{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"erisk_client.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"eXaGu26U0A4g","colab_type":"code","outputId":"73d89acb-2980-48d4-cc62-2ffb72cb9625","executionInfo":{"status":"ok","timestamp":1560447379373,"user_tz":-120,"elapsed":592,"user":{"displayName":"PABLO RAEZ GARCIA-RETAMERO","photoUrl":"","userId":"03506494368259470344"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mNB37wWuzMnP","colab_type":"code","colab":{}},"source":["import requests\n","import pandas as pd\n","import os\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NfZLnIE86OEm","colab_type":"code","colab":{}},"source":["TEAM_TOKEN = '1f4c93c4e114d6e72706c31901101db7vv'\n","SERVER_URL_TASK1 = 'http://erisk.irlab.org/challenge-t1e/getwritings/' + TEAM_TOKEN\n","SERVER_URL_TASK2 = 'http://erisk.irlab.org/challenge-t2/getwritings/' + TEAM_TOKEN"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ixwV-rfJlezP","colab_type":"code","colab":{}},"source":["SST_HOME='drive/My Drive/Colab Notebooks/Erisk2019/'\n","PATH_SERVER_TASK1=SST_HOME+'datas/unoficcial server data/task1/test.csv'\n","PATH_SERVER_TASK2=SST_HOME+'datas/unoficcial server data/task2/test.csv'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4hBj7bZ6nacw","colab_type":"code","colab":{}},"source":["def ask_server_df(url):\n","  \"\"\"\n","  This function performs a get request to the servers and returns the data as a dataframe.\n","  \n","  Returns None if any problem occurs.\n","  \"\"\"\n","  r = requests.get(url)\n","  if r.status_code == 200:\n","    df = pd.DataFrame(r.json())\n","    df = df.rename(index=str, columns={\"nick\": \"ID\", \"date\": \"DATE\", \"title\":\"TITLE\", \"redditor\":\"INFO\", \"content\":\"TEXT\"})\n","    df = df.set_index(['ID', 'DATE'])\n","    return df\n","  else:\n","    return None"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q466uvA0oclq","colab_type":"code","colab":{}},"source":["df_t1 = ask_server_df(SERVER_URL_TASK1)\n","# df_t2 = ask_server_df(SERVER_URL_TASK2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8fJbz4SN6Xlr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1979},"outputId":"42a894e2-7d7e-4c6d-ff80-1feba24b1d0e","executionInfo":{"status":"ok","timestamp":1560447388842,"user_tz":-120,"elapsed":500,"user":{"displayName":"PABLO RAEZ GARCIA-RETAMERO","photoUrl":"","userId":"03506494368259470344"}}},"source":["df_t1"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>TEXT</th>\n","      <th>id</th>\n","      <th>number</th>\n","      <th>INFO</th>\n","      <th>TITLE</th>\n","    </tr>\n","    <tr>\n","      <th>ID</th>\n","      <th>DATE</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>subject1001</th>\n","      <th>2015-09-11T22:56:44.000+0000</th>\n","      <td>got it</td>\n","      <td>382143</td>\n","      <td>395</td>\n","      <td>365324</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1044</th>\n","      <th>2016-04-13T07:55:01.000+0000</th>\n","      <td>It did actually! Just jabjabjab!</td>\n","      <td>320240</td>\n","      <td>395</td>\n","      <td>297535</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1074</th>\n","      <th>2017-02-06T21:05:32.000+0000</th>\n","      <td>Came here to say the same thing lol. The stup...</td>\n","      <td>233012</td>\n","      <td>395</td>\n","      <td>224363</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1157</th>\n","      <th>2014-03-19T17:24:03.000+0000</th>\n","      <td>Please tell me there's a higher-resolution or...</td>\n","      <td>465293</td>\n","      <td>395</td>\n","      <td>432096</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1164</th>\n","      <th>2018-02-01T12:03:57.000+0000</th>\n","      <td>I see people are more accepting of apple now</td>\n","      <td>482997</td>\n","      <td>395</td>\n","      <td>468872</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1165</th>\n","      <th>2016-04-12T11:32:54.000+0000</th>\n","      <td></td>\n","      <td>477063</td>\n","      <td>395</td>\n","      <td>468868</td>\n","      <td>NYTimes: Is the Worlds Best Croissant Made in...</td>\n","    </tr>\n","    <tr>\n","      <th>subject1171</th>\n","      <th>2018-01-10T02:52:47.000+0000</th>\n","      <td>Now, defense Jayhawks</td>\n","      <td>239066</td>\n","      <td>395</td>\n","      <td>224370</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1174</th>\n","      <th>2017-05-12T00:13:50.000+0000</th>\n","      <td>Protip: if you want to search for something ...</td>\n","      <td>475224</td>\n","      <td>395</td>\n","      <td>468864</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1183</th>\n","      <th>2018-03-31T04:07:38.000+0000</th>\n","      <td>What games did you end up trying?</td>\n","      <td>508389</td>\n","      <td>395</td>\n","      <td>507953</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1208</th>\n","      <th>2016-09-21T18:18:47.000+0000</th>\n","      <td>It got Solar beam! :D</td>\n","      <td>447</td>\n","      <td>395</td>\n","      <td>1</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1224</th>\n","      <th>2014-08-05T23:50:11.000+0000</th>\n","      <td>Churches are free to endorse politicians from...</td>\n","      <td>167821</td>\n","      <td>395</td>\n","      <td>150024</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1260</th>\n","      <th>2017-07-28T14:19:43.000+0000</th>\n","      <td>Just send me an email at lysanderxonoraoffici...</td>\n","      <td>152582</td>\n","      <td>395</td>\n","      <td>150004</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject130</th>\n","      <th>2017-10-08T21:05:47.000+0000</th>\n","      <td>The dusty winds of Oklahoma beg to differ</td>\n","      <td>534928</td>\n","      <td>395</td>\n","      <td>507998</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1306</th>\n","      <th>2016-04-30T16:23:16.000+0000</th>\n","      <td></td>\n","      <td>230573</td>\n","      <td>395</td>\n","      <td>224361</td>\n","      <td>Interview with Microsofts Lead Producer on Mi...</td>\n","    </tr>\n","    <tr>\n","      <th>subject1315</th>\n","      <th>2017-10-20T22:00:45.000+0000</th>\n","      <td></td>\n","      <td>496645</td>\n","      <td>395</td>\n","      <td>468891</td>\n","      <td>Whats your favorite bands worst album?</td>\n","    </tr>\n","    <tr>\n","      <th>subject1335</th>\n","      <th>2016-03-28T18:57:56.000+0000</th>\n","      <td>I think given his acting past, it's more like...</td>\n","      <td>301080</td>\n","      <td>395</td>\n","      <td>297511</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject136</th>\n","      <th>2016-05-28T01:09:25.000+0000</th>\n","      <td>About twice as big as it would appear to be ...</td>\n","      <td>544105</td>\n","      <td>395</td>\n","      <td>537660</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1367</th>\n","      <th>2018-04-11T17:09:49.000+0000</th>\n","      <td>Her name is Stav Shaffir, shes the youngest m...</td>\n","      <td>512625</td>\n","      <td>395</td>\n","      <td>507961</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1370</th>\n","      <th>2016-01-06T20:06:42.000+0000</th>\n","      <td>And therefore useless in other countries. Can...</td>\n","      <td>509907</td>\n","      <td>395</td>\n","      <td>507957</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject139</th>\n","      <th>2018-04-02T00:23:34.000+0000</th>\n","      <td>u guys make me sad</td>\n","      <td>408707</td>\n","      <td>395</td>\n","      <td>401113</td>\n","      <td>why yall always ignore my posts</td>\n","    </tr>\n","    <tr>\n","      <th>subject1398</th>\n","      <th>2018-03-25T20:46:41.000+0000</th>\n","      <td>Catpoeira</td>\n","      <td>251834</td>\n","      <td>395</td>\n","      <td>224385</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1416</th>\n","      <th>2018-01-07T02:28:28.000+0000</th>\n","      <td>Then it'd just be a</td>\n","      <td>75853</td>\n","      <td>395</td>\n","      <td>40247</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1446</th>\n","      <th>2018-01-29T20:26:51.000+0000</th>\n","      <td>r/picturesofiansleeping</td>\n","      <td>321575</td>\n","      <td>395</td>\n","      <td>297538</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1448</th>\n","      <th>2017-04-28T16:24:04.000+0000</th>\n","      <td></td>\n","      <td>148381</td>\n","      <td>395</td>\n","      <td>122651</td>\n","      <td>What's the best band with the worst live perf...</td>\n","    </tr>\n","    <tr>\n","      <th>subject1466</th>\n","      <th>2017-04-12T12:04:29.000+0000</th>\n","      <td></td>\n","      <td>1994</td>\n","      <td>395</td>\n","      <td>4</td>\n","      <td>Jodeci - Lately [R B]</td>\n","    </tr>\n","    <tr>\n","      <th>subject1513</th>\n","      <th>2015-01-08T23:46:05.000+0000</th>\n","      <td>You can simply downvote the ad and it will st...</td>\n","      <td>232157</td>\n","      <td>395</td>\n","      <td>224362</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1537</th>\n","      <th>2015-06-19T17:53:54.000+0000</th>\n","      <td>I know it says minimal cuts, but I figure I'...</td>\n","      <td>3645</td>\n","      <td>395</td>\n","      <td>5</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1588</th>\n","      <th>2017-10-13T21:39:38.000+0000</th>\n","      <td>I am sure that this was a much more common an...</td>\n","      <td>246752</td>\n","      <td>395</td>\n","      <td>224380</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1593</th>\n","      <th>2018-02-04T18:22:01.000+0000</th>\n","      <td>Get laid facker</td>\n","      <td>495065</td>\n","      <td>395</td>\n","      <td>468888</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject1627</th>\n","      <th>2014-02-07T22:34:36.000+0000</th>\n","      <td></td>\n","      <td>4393</td>\n","      <td>395</td>\n","      <td>7</td>\n","      <td>Russian Police Choir performs 'Get Lucky' at ...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>subject9499</th>\n","      <th>2016-07-01T11:37:59.000+0000</th>\n","      <td>EDIT: \"First\" that I remember, at least.  So...</td>\n","      <td>289914</td>\n","      <td>395</td>\n","      <td>262139</td>\n","      <td>Had my first dream of eating!</td>\n","    </tr>\n","    <tr>\n","      <th>subject9572</th>\n","      <th>2014-11-21T21:07:04.000+0000</th>\n","      <td>maybe we hung out once.</td>\n","      <td>369022</td>\n","      <td>395</td>\n","      <td>365308</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9578</th>\n","      <th>2016-07-14T13:39:46.000+0000</th>\n","      <td>Did nazi that coming</td>\n","      <td>217882</td>\n","      <td>395</td>\n","      <td>188293</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9623</th>\n","      <th>2018-01-20T18:15:40.000+0000</th>\n","      <td>That's the same link I provided....</td>\n","      <td>450259</td>\n","      <td>395</td>\n","      <td>432076</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9627</th>\n","      <th>2018-05-01T19:42:40.000+0000</th>\n","      <td>Earl says to the Bubba, \"Have you seen the b...</td>\n","      <td>567389</td>\n","      <td>395</td>\n","      <td>564907</td>\n","      <td>Two young rednecks were looking at a Sears ca...</td>\n","    </tr>\n","    <tr>\n","      <th>subject9633</th>\n","      <th>2016-12-17T03:59:39.000+0000</th>\n","      <td>Man I wish there was something even half as c...</td>\n","      <td>515572</td>\n","      <td>395</td>\n","      <td>507964</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9652</th>\n","      <th>2015-12-01T15:01:27.000+0000</th>\n","      <td>I just use a 20mm. It's not the *best* but i...</td>\n","      <td>370101</td>\n","      <td>395</td>\n","      <td>365310</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9656</th>\n","      <th>2018-03-27T15:33:38.000+0000</th>\n","      <td>Has anyone gone through this? I can't even f...</td>\n","      <td>371549</td>\n","      <td>395</td>\n","      <td>365311</td>\n","      <td>Notice of Issuance of Citation - HCTRA toll t...</td>\n","    </tr>\n","    <tr>\n","      <th>subject9659</th>\n","      <th>2016-11-20T20:59:23.000+0000</th>\n","      <td></td>\n","      <td>218625</td>\n","      <td>395</td>\n","      <td>188294</td>\n","      <td>Sec. Kerry on the Cessation of Hostilities in...</td>\n","    </tr>\n","    <tr>\n","      <th>subject9671</th>\n","      <th>2017-03-31T07:16:51.000+0000</th>\n","      <td></td>\n","      <td>372131</td>\n","      <td>395</td>\n","      <td>365312</td>\n","      <td>\"SwissGames at GDC 2017\" - most of our visits...</td>\n","    </tr>\n","    <tr>\n","      <th>subject9675</th>\n","      <th>2017-10-05T15:22:44.000+0000</th>\n","      <td></td>\n","      <td>290824</td>\n","      <td>395</td>\n","      <td>262142</td>\n","      <td>The Green Children - Feel The Light [Electrop...</td>\n","    </tr>\n","    <tr>\n","      <th>subject971</th>\n","      <th>2014-12-13T09:09:27.000+0000</th>\n","      <td>Thanks for the answer. I'll try out this syst...</td>\n","      <td>374365</td>\n","      <td>395</td>\n","      <td>365314</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9726</th>\n","      <th>2017-01-04T13:31:55.000+0000</th>\n","      <td>No idea, just tryed it again myself and it wo...</td>\n","      <td>517151</td>\n","      <td>395</td>\n","      <td>507966</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject973</th>\n","      <th>2015-11-02T05:36:29.000+0000</th>\n","      <td>Are you going to make a profound statement ab...</td>\n","      <td>375136</td>\n","      <td>395</td>\n","      <td>365315</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9734</th>\n","      <th>2018-02-01T23:18:31.000+0000</th>\n","      <td></td>\n","      <td>377133</td>\n","      <td>395</td>\n","      <td>365316</td>\n","      <td>Free Your Media: How to Build a Home Media Se...</td>\n","    </tr>\n","    <tr>\n","      <th>subject9777</th>\n","      <th>2016-09-12T02:17:49.000+0000</th>\n","      <td>Take some Max Strength Imodium. Will help big...</td>\n","      <td>567924</td>\n","      <td>395</td>\n","      <td>564910</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9787</th>\n","      <th>2017-02-11T13:09:09.000+0000</th>\n","      <td>Do it now.</td>\n","      <td>144354</td>\n","      <td>395</td>\n","      <td>122639</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9796</th>\n","      <th>2013-09-08T12:58:25.000+0000</th>\n","      <td></td>\n","      <td>452256</td>\n","      <td>395</td>\n","      <td>432077</td>\n","      <td>30 Days of Awesome: Chase Adam of Watsi.org</td>\n","    </tr>\n","    <tr>\n","      <th>subject980</th>\n","      <th>2017-03-22T19:46:43.000+0000</th>\n","      <td></td>\n","      <td>379267</td>\n","      <td>395</td>\n","      <td>365318</td>\n","      <td>Oasis - Dont Look Back In Anger[Brit Pop]</td>\n","    </tr>\n","    <tr>\n","      <th>subject9803</th>\n","      <th>2017-09-07T22:46:13.000+0000</th>\n","      <td>Coming for Best Pop Vocal Performance Grammy,...</td>\n","      <td>219812</td>\n","      <td>395</td>\n","      <td>188296</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9837</th>\n","      <th>2016-05-15T21:47:33.000+0000</th>\n","      <td>That's the best part. Also the elevator goes ...</td>\n","      <td>517649</td>\n","      <td>395</td>\n","      <td>507967</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9856</th>\n","      <th>2018-02-26T15:58:40.000+0000</th>\n","      <td>Knew something was wrong when I saw an Alex G...</td>\n","      <td>568944</td>\n","      <td>395</td>\n","      <td>564913</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9858</th>\n","      <th>2017-11-02T17:56:34.000+0000</th>\n","      <td>It's treason then.</td>\n","      <td>518252</td>\n","      <td>395</td>\n","      <td>507968</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9859</th>\n","      <th>2015-02-05T23:36:11.000+0000</th>\n","      <td>A big painting like [CITY] (http://joshbyer.d...</td>\n","      <td>570370</td>\n","      <td>395</td>\n","      <td>564914</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9923</th>\n","      <th>2018-04-09T03:05:54.000+0000</th>\n","      <td></td>\n","      <td>221583</td>\n","      <td>395</td>\n","      <td>188300</td>\n","      <td>WeWatchedAMovie on Instagram: I am LOVING the...</td>\n","    </tr>\n","    <tr>\n","      <th>subject9927</th>\n","      <th>2015-11-10T22:25:30.000+0000</th>\n","      <td>Today we released a new feature: [account su...</td>\n","      <td>70750</td>\n","      <td>395</td>\n","      <td>40239</td>\n","      <td>[reddit change] account suspensions</td>\n","    </tr>\n","    <tr>\n","      <th>subject9929</th>\n","      <th>2013-09-28T21:47:24.000+0000</th>\n","      <td></td>\n","      <td>72231</td>\n","      <td>395</td>\n","      <td>40240</td>\n","      <td>Took a nap, only to be woken up to Milo's \"ha...</td>\n","    </tr>\n","    <tr>\n","      <th>subject996</th>\n","      <th>2018-03-27T13:05:47.000+0000</th>\n","      <td>Why would Cohen put a signature line for DD a...</td>\n","      <td>223581</td>\n","      <td>395</td>\n","      <td>188301</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject998</th>\n","      <th>2018-03-01T10:28:55.000+0000</th>\n","      <td>FORTNITE!  Jk I hate that and PUBG players w...</td>\n","      <td>454115</td>\n","      <td>395</td>\n","      <td>432078</td>\n","      <td></td>\n","    </tr>\n","    <tr>\n","      <th>subject9997</th>\n","      <th>2017-07-03T23:53:48.000+0000</th>\n","      <td></td>\n","      <td>293213</td>\n","      <td>395</td>\n","      <td>262145</td>\n","      <td>Venezuelas Poor Rebel, Roiling Maduros Social...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>432 rows Ã— 5 columns</p>\n","</div>"],"text/plain":["                                                                                       TEXT  ...                                              TITLE\n","ID          DATE                                                                             ...                                                   \n","subject1001 2015-09-11T22:56:44.000+0000                                            got it   ...                                                   \n","subject1044 2016-04-13T07:55:01.000+0000                  It did actually! Just jabjabjab!   ...                                                   \n","subject1074 2017-02-06T21:05:32.000+0000   Came here to say the same thing lol. The stup...  ...                                                   \n","subject1157 2014-03-19T17:24:03.000+0000   Please tell me there's a higher-resolution or...  ...                                                   \n","subject1164 2018-02-01T12:03:57.000+0000      I see people are more accepting of apple now   ...                                                   \n","subject1165 2016-04-12T11:32:54.000+0000                                                     ...   NYTimes: Is the Worlds Best Croissant Made in...\n","subject1171 2018-01-10T02:52:47.000+0000                            Now, defense Jayhawks    ...                                                   \n","subject1174 2017-05-12T00:13:50.000+0000    Protip: if you want to search for something ...  ...                                                   \n","subject1183 2018-03-31T04:07:38.000+0000                 What games did you end up trying?   ...                                                   \n","subject1208 2016-09-21T18:18:47.000+0000                             It got Solar beam! :D   ...                                                   \n","subject1224 2014-08-05T23:50:11.000+0000   Churches are free to endorse politicians from...  ...                                                   \n","subject1260 2017-07-28T14:19:43.000+0000   Just send me an email at lysanderxonoraoffici...  ...                                                   \n","subject130  2017-10-08T21:05:47.000+0000         The dusty winds of Oklahoma beg to differ   ...                                                   \n","subject1306 2016-04-30T16:23:16.000+0000                                                     ...   Interview with Microsofts Lead Producer on Mi...\n","subject1315 2017-10-20T22:00:45.000+0000                                                     ...            Whats your favorite bands worst album? \n","subject1335 2016-03-28T18:57:56.000+0000   I think given his acting past, it's more like...  ...                                                   \n","subject136  2016-05-28T01:09:25.000+0000    About twice as big as it would appear to be ...  ...                                                   \n","subject1367 2018-04-11T17:09:49.000+0000   Her name is Stav Shaffir, shes the youngest m...  ...                                                   \n","subject1370 2016-01-06T20:06:42.000+0000   And therefore useless in other countries. Can...  ...                                                   \n","subject139  2018-04-02T00:23:34.000+0000                                u guys make me sad   ...                   why yall always ignore my posts \n","subject1398 2018-03-25T20:46:41.000+0000                                         Catpoeira   ...                                                   \n","subject1416 2018-01-07T02:28:28.000+0000                              Then it'd just be a    ...                                                   \n","subject1446 2018-01-29T20:26:51.000+0000                           r/picturesofiansleeping   ...                                                   \n","subject1448 2017-04-28T16:24:04.000+0000                                                     ...   What's the best band with the worst live perf...\n","subject1466 2017-04-12T12:04:29.000+0000                                                     ...                             Jodeci - Lately [R B] \n","subject1513 2015-01-08T23:46:05.000+0000   You can simply downvote the ad and it will st...  ...                                                   \n","subject1537 2015-06-19T17:53:54.000+0000    I know it says minimal cuts, but I figure I'...  ...                                                   \n","subject1588 2017-10-13T21:39:38.000+0000   I am sure that this was a much more common an...  ...                                                   \n","subject1593 2018-02-04T18:22:01.000+0000                                  Get laid facker    ...                                                   \n","subject1627 2014-02-07T22:34:36.000+0000                                                     ...   Russian Police Choir performs 'Get Lucky' at ...\n","...                                                                                     ...  ...                                                ...\n","subject9499 2016-07-01T11:37:59.000+0000    EDIT: \"First\" that I remember, at least.  So...  ...                     Had my first dream of eating! \n","subject9572 2014-11-21T21:07:04.000+0000                           maybe we hung out once.   ...                                                   \n","subject9578 2016-07-14T13:39:46.000+0000                              Did nazi that coming   ...                                                   \n","subject9623 2018-01-20T18:15:40.000+0000               That's the same link I provided....   ...                                                   \n","subject9627 2018-05-01T19:42:40.000+0000    Earl says to the Bubba, \"Have you seen the b...  ...   Two young rednecks were looking at a Sears ca...\n","subject9633 2016-12-17T03:59:39.000+0000   Man I wish there was something even half as c...  ...                                                   \n","subject9652 2015-12-01T15:01:27.000+0000    I just use a 20mm. It's not the *best* but i...  ...                                                   \n","subject9656 2018-03-27T15:33:38.000+0000    Has anyone gone through this? I can't even f...  ...   Notice of Issuance of Citation - HCTRA toll t...\n","subject9659 2016-11-20T20:59:23.000+0000                                                     ...   Sec. Kerry on the Cessation of Hostilities in...\n","subject9671 2017-03-31T07:16:51.000+0000                                                     ...   \"SwissGames at GDC 2017\" - most of our visits...\n","subject9675 2017-10-05T15:22:44.000+0000                                                     ...   The Green Children - Feel The Light [Electrop...\n","subject971  2014-12-13T09:09:27.000+0000   Thanks for the answer. I'll try out this syst...  ...                                                   \n","subject9726 2017-01-04T13:31:55.000+0000   No idea, just tryed it again myself and it wo...  ...                                                   \n","subject973  2015-11-02T05:36:29.000+0000   Are you going to make a profound statement ab...  ...                                                   \n","subject9734 2018-02-01T23:18:31.000+0000                                                     ...   Free Your Media: How to Build a Home Media Se...\n","subject9777 2016-09-12T02:17:49.000+0000   Take some Max Strength Imodium. Will help big...  ...                                                   \n","subject9787 2017-02-11T13:09:09.000+0000                                        Do it now.   ...                                                   \n","subject9796 2013-09-08T12:58:25.000+0000                                                     ...       30 Days of Awesome: Chase Adam of Watsi.org \n","subject980  2017-03-22T19:46:43.000+0000                                                     ...         Oasis - Dont Look Back In Anger[Brit Pop] \n","subject9803 2017-09-07T22:46:13.000+0000   Coming for Best Pop Vocal Performance Grammy,...  ...                                                   \n","subject9837 2016-05-15T21:47:33.000+0000   That's the best part. Also the elevator goes ...  ...                                                   \n","subject9856 2018-02-26T15:58:40.000+0000   Knew something was wrong when I saw an Alex G...  ...                                                   \n","subject9858 2017-11-02T17:56:34.000+0000                                It's treason then.   ...                                                   \n","subject9859 2015-02-05T23:36:11.000+0000   A big painting like [CITY] (http://joshbyer.d...  ...                                                   \n","subject9923 2018-04-09T03:05:54.000+0000                                                     ...   WeWatchedAMovie on Instagram: I am LOVING the...\n","subject9927 2015-11-10T22:25:30.000+0000    Today we released a new feature: [account su...  ...               [reddit change] account suspensions \n","subject9929 2013-09-28T21:47:24.000+0000                                                     ...   Took a nap, only to be woken up to Milo's \"ha...\n","subject996  2018-03-27T13:05:47.000+0000   Why would Cohen put a signature line for DD a...  ...                                                   \n","subject998  2018-03-01T10:28:55.000+0000    FORTNITE!  Jk I hate that and PUBG players w...  ...                                                   \n","subject9997 2017-07-03T23:53:48.000+0000                                                     ...   Venezuelas Poor Rebel, Roiling Maduros Social...\n","\n","[432 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"esPatyiX-QeH","colab_type":"code","colab":{}},"source":["def append_to_df(path, df, copy=True):\n","  \"\"\"\n","  This function appends the new recorded data to an already existing csv. \n","  \n","  If copy is set to True (default) it creates a copy of the csv files.\n","  \"\"\"\n","  \n","  if os.path.exists(path):\n","    df2 = pd.read_csv(path, index_col=['ID', 'DATE'])\n","    if copy:\n","      df2.to_csv(path[:-4]+'_old'+path[-4:])\n","      \n","    df = df.append(df2).drop_duplicates()\n","    df.to_csv(path)\n","    \n","  else:\n","    df.to_csv(path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nzZQ7kcBFusY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":433},"outputId":"592698c2-96c8-48d4-ec15-bc0c5ba40f1e","executionInfo":{"status":"error","timestamp":1560447405013,"user_tz":-120,"elapsed":4475,"user":{"displayName":"PABLO RAEZ GARCIA-RETAMERO","photoUrl":"","userId":"03506494368259470344"}}},"source":["r = requests.get(SERVER_URL_TASK1)"],"execution_count":9,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-0ee0cbebb513>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSERVER_URL_TASK1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"3Xxa8IQzFy1_","colab_type":"code","colab":{}},"source":["r.body"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eQOSla6JFXrR","colab_type":"code","colab":{}},"source":["df2 = pd.read_csv(PATH_SERVER_TASK1, index_col=['ID', 'DATE'])\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MgIok_iMFmE2","colab_type":"code","colab":{}},"source":["len(df2), len(df_t1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MGCSAT-cyolx","colab_type":"code","colab":{}},"source":["append_to_df(PATH_SERVER_TASK1, df_t1, copy=False)\n","# append_to_df(PATH_SERVER_TASK2, df_t2, copy=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K-nFyIUpTe1-","colab_type":"text"},"source":["Load and Ready Data\n","================="]},{"cell_type":"code","metadata":{"id":"yCtl-6hLTedJ","colab_type":"code","colab":{}},"source":["records_df = pd.read_csv(PATH_SERVER_TASK1)\n","records_df = records_df.set_index(['ID', 'DATE'])\n","records_df[\"TITE\"] = records_df[\"TEXT\"] + records_df[\"TITLE\"]\n","\n","\n","records_x_subject = []\n","for subject in records_df.index.get_level_values(0).unique():\n","  records_x_subject.append(records_df.loc[subject][\"TITE\"].tolist())\n","  \n","records_x_simple = []\n","for subject in records_x_subject:\n","  for writting in subject:\n","    records_x_simple.append(writting)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_D4Q9ZAtUjiK","colab_type":"code","colab":{}},"source":["len(records_x_simple), len(records_x_subject)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C8xo1ErbhLws","colab_type":"text"},"source":["Tokenize\n","========"]},{"cell_type":"code","metadata":{"id":"wlTUskKxqlO-","colab_type":"code","colab":{}},"source":["from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","stop_words=stopwords.words('english')\n","# Define maximum vocabulary length\n","MAX_WORDS = 5000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wVPBM3kJwtaK","colab_type":"code","colab":{}},"source":["# We are using this function to clean the test set\n","def tokenize_clean_text(text, tfidf=True, tokenizer=None, max_length=None, max_words=MAX_WORDS):\n","  \"\"\"\n","  This function is in charge of tokenizing the text it is given. It also cleans\n","  the text from stop-words, punctuation, and gives a special token to numbers.\n","  \n","  :param text: The texts to tokenize in a bidimensional python array.\n","  \n","  :returns: The tokenized and cleaned text in a bidimensional python array.\n","            The tokenizer used to preprocess the text.\n","            The maximum length used for padding.\n","  \"\"\"   \n","  # set [removed] as a special token\n","  text_removed = [t.replace(\"[removed]\", \"R3MOV3D\") for t in text]\n","  \n","  # We remove the numbers\n","  cropped_numbers_text = [\" \".join([word if not word.isdigit() else \"\"\n","                                for word in sentence.split()])\n","                               for sentence in text_removed]\n","  \n","  # Delete stopwords as well as every word less than 3 chars.\n","  cropped_numbers_stopw_text = [\" \".join([word if not (word in stop_words or len(word) <= 3) else \"\"\n","                                      for word in sentence.split()])\n","                                     for sentence in cropped_numbers_text]\n","  \n","  if tfidf:\n","    vec = TfidfVectorizer(max_features=max_words)\n","    tfidf_mat = vec.fit_transform(cropped_numbers_stopw_text).toarray()\n","    tfid_words = vec.get_feature_names()\n","\n","    cropped_numbers_stopw_tfidf_text = [\" \".join([word if word in tfid_words else \"\"\n","                                            for word in sentence.split()])\n","                                            for sentence in cropped_numbers_stopw_text]\n","  \n","  if tokenizer is None:\n","    tokenizer = Tokenizer(num_words=max_words) # They use 5k words too\n","    tokenizer.fit_on_texts(cropped_numbers_stopw_tfidf_text if tfidf else cropped_numbers_stopw_text)\n","  # We tokenize the sentences\n","  tokenized_text = tokenizer.texts_to_sequences(cropped_numbers_stopw_tfidf_text if tfidf else cropped_numbers_stopw_text)\n","  \n","  if max_length == None:\n","    max_length = 0\n","    for sentence in tokenized_text:\n","      max_length = max_length if len(sentence) < max_length else len(sentence)\n","  \n","  # Now we return the padded the sequences.\n","  return pad_sequences(tokenized_text, max_length), tokenizer, max_length, cropped_numbers_stopw_tfidf_text if tfidf else cropped_numbers_stopw_text\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HjFuz5GZmHR8","colab_type":"code","colab":{}},"source":["# load the tokenizer\n","import joblib\n","PATH_CHAR_TOKENIZER = SST_HOME + \"DL/char_tokenizer.pkl\"\n","PATH_TOKENIZER = SST_HOME + \"DL/tokenizer.pkl\"\n","tokenizer = joblib.load(PATH_TOKENIZER)\n","char_tokenizer = joblib.load(PATH_CHAR_TOKENIZER)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UARLdwsDya70","colab_type":"code","colab":{}},"source":["records_x_token, _, _, results_x_clean = tokenize_clean_text(records_x_simple, tokenizer=tokenizer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2Q6xmWNUsri","colab_type":"code","colab":{}},"source":["# the token length will be set to 50\n","max_length = 50\n","records_x_token_crop = records_x_token[:,-max_length:]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3vOG-RWbvNR8","colab_type":"code","colab":{}},"source":["records_df[\"TOKENIZED\"] = records_x_token_crop.tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-LqqKjr3oL_-","colab_type":"code","colab":{}},"source":["results_x_char = [[c for c in instance] for instance in results_x_clean]\n","# the char length will be set to 50\n","max_char_length = 400\n","\n","results_x_char_padded = [[\".\"]*(max_char_length - len(instance)) + instance if len(instance) < max_char_length else instance for instance in results_x_char]\n","results_x_char_crop = [instance[-max_char_length:] for instance in results_x_char_padded]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"C1iuB_vYaib4","colab_type":"code","colab":{}},"source":["def tokenize_chars(text, tokenizer=None, max_length=max_char_length):\n","  if tokenizer is None:\n","    tokenizer = Tokenizer() # They use 5k words too\n","    tokenizer.fit_on_texts(text)\n","  # We tokenize the sentences\n","  tokenized_text = tokenizer.texts_to_sequences(text)\n","  \n","  for i, t in enumerate(tokenized_text):\n","    if len(t) < max_length:\n","      tokenized_text[i] = [1] * (max_length - len(t)) + tokenized_text[i]\n","      \n","  return tokenized_text, tokenizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eB7NXAFQrBCa","colab_type":"code","colab":{}},"source":["results_x_char_crop_token, _ = tokenize_chars(results_x_char_crop, char_tokenizer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9C6icfWZvZtb","colab_type":"code","colab":{}},"source":["records_df[\"TOK_CHAR\"] = results_x_char_crop_token"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ADeiA9Nwvgir","colab_type":"text"},"source":["Preprocess with A and D models\n","=========================="]},{"cell_type":"code","metadata":{"id":"gcHEowdhPkEv","colab_type":"code","colab":{}},"source":["A_MODELS_PATH = SST_HOME + \"DL/models/A/best_models/\"\n","B_MODELS_PATH = SST_HOME + \"DL/models/B/best_models/\"\n","C_MODELS_PATH = SST_HOME + \"DL/models/C/best_models/\"\n","D_MODELS_PATH = SST_HOME + \"DL/models/D/best_models/\"\n","E_MODELS_PATH = SST_HOME + \"DL/models/E/best_models/\"\n","\n","\n","A_MODEL_PATH = A_MODELS_PATH + \"Copia de load_emb_False_num_classes_1_emb_size_300_trainable_emb_True_cnn_size_128_cnn_filter_3_rnn_size_None_cell_type_LSTM_bidirectional_False_attention_False_dropout_0.5_dnn_size_32_batch_size_1024\"\n","B_MODEL_PATH = B_MODELS_PATH + \"Copia de load_emb_False_num_classes_1_emb_size_300_trainable_emb_True_cnn_size_None_cnn_filter_3_rnn_size_64_cell_type_GRU_bidirectional_True_attention_False_dropout_0.5_dnn_size_32_batch_size_None\"\n","C_MODEL_PATH = C_MODELS_PATH + \"Copia de load_emb_False_num_classes_1_emb_size_None_trainable_emb_False_cnn_size_None_cnn_filter_3_rnn_size_64_cell_type_LSTM_bidirectional_True_attention_False_dropout_0.5_dnn_size_32_batch_size_1\"\n","D_MODEL_PATH = D_MODELS_PATH + \"Copia de load_emb_False_num_classes_1_emb_size_50_trainable_emb_True_cnn_size_128_cnn_filter_10_rnn_size_None_cell_type_LSTM_bidirectional_False_attention_False_dropout_0.5_dnn_size_64_batch_size_1024\"\n","E_MODEL_PATH = E_MODELS_PATH + \"Copia de load_emb_False_num_classes_1_emb_size_None_trainable_emb_False_cnn_size_None_cnn_filter_3_rnn_size_64_cell_type_GRU_bidirectional_False_attention_False_dropout_0.5_dnn_size_32_batch_size_1\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6-rlpiwrvgG9","colab_type":"code","colab":{}},"source":["from keras.models import load_model\n","from keras.models import Sequential"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yLlEE_S6p4s0","colab_type":"code","colab":{}},"source":["a_model = load_model(A_MODEL_PATH)\n","a_model.layers.pop()\n","a_model.layers.pop()\n","# We pop two layers, being them the output and the last Dropout layers\n","c_pre_model = Sequential()\n","\n","for layer in a_model.layers:\n","  c_pre_model.add(layer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L5sLbpR10rDm","colab_type":"code","colab":{}},"source":["d_model = load_model(D_MODEL_PATH)\n","d_model.layers.pop()\n","d_model.layers.pop()\n","# We pop two layers, being them the output and the last dropout ones\n","e_pre_model = Sequential()\n","\n","for layer in d_model.layers:\n","  e_pre_model.add(layer)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GztJmHAtX0dI","colab_type":"code","colab":{}},"source":["instances = np.array(records_df[\"TOKENIZED\"].tolist())\n","char_instances = np.array(records_df[\"TOK_CHAR\"].tolist())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FazNzbJ5P_qY","colab_type":"code","colab":{}},"source":["embedded_text = c_pre_model.predict(instances)\n","embedded_chars = e_pre_model.predict(char_instances)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NrJN1sd-dRt6","colab_type":"code","colab":{}},"source":["records_df[\"EMBEDDINGS\"] = embedded_text.tolist()\n","records_df[\"CHAR_EMBED\"] = embedded_chars.tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GB_bgNdes19A","colab_type":"code","colab":{}},"source":["records_subjects_stacked = []\n","records_subjects_embeddings = []\n","records_subjects_char_embeddings = []\n","\n","for subject in records_df.index.get_level_values(0).unique():\n","  # B model\n","  for writting in records_df.loc[subject][\"TOKENIZED\"].tolist():\n","    while 0 in writting:\n","      writting.remove(0)\n","    writting = writting[-350:]\n","    if len(writting) < 350:\n","      writting = [0]*(350 - len(writting)) + writting\n","  records_subjects_stacked.append(writting)\n","  records_subjects_embeddings.append(records_df.loc[subject][\"EMBEDDINGS\"].tolist())\n","  records_subjects_char_embeddings.append(records_df.loc[subject][\"CHAR_EMBED\"].tolist())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6j4ji6TyIilY","colab_type":"code","colab":{}},"source":["records_df[\"TOKENIZED\"] = records_x_token_crop.tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lHap7xY_PkmR","colab_type":"text"},"source":["Load Models\n","==========="]},{"cell_type":"code","metadata":{"id":"9dsTJKwd07TJ","colab_type":"code","colab":{}},"source":["# load_models\n","a_model = load_model(A_MODEL_PATH)\n","b_model = load_model(B_MODEL_PATH)\n","c_model = load_model(C_MODEL_PATH)\n","d_model = load_model(D_MODEL_PATH)\n","e_model = load_model(E_MODEL_PATH)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lFT0SBPUV3C3","colab_type":"code","colab":{}},"source":["e_model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNXZP6PMM81X","colab_type":"code","colab":{}},"source":["32records_df[\"AVALUE\"] = np.nan\n","records_df[\"BVALUE\"] = np.nan\n","records_df[\"CVALUE\"] = np.nan\n","records_df[\"DVALUE\"] = np.nan\n","records_df[\"EVALUE\"] = np.nan"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nyGE8lzNiGFZ","colab_type":"code","colab":{}},"source":["# Predictions\n","\n","# A model\n","records_df[\"AVALUE\"] = a_model.predict(np.array(records_df[\"TOKENIZED\"].values.tolist()))\n","print(\"A done\")\n","\n","# B model\n","b_predictions = b_model.predict(np.array(records_subjects_stacked))\n","print(\"B done\")\n","\n","# C model\n","c_predictions = [c_model.predict(np.array([s_c])) for s_c in records_subjects_embeddings]\n","print(\"C done\")\n","\n","# D model\n","records_df[\"DVALUE\"] = d_model.predict(np.array(records_df[\"TOK_CHAR\"].values.tolist()))\n","print(\"D done\")\n","\n","# E model\n","e_predictions = [e_model.predict(np.array([s_e])) for s_e in records_subjects_char_embeddings]\n","print(\"E done\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UG6ocMZb1IEW","colab_type":"text"},"source":["Subjects results\n","============="]},{"cell_type":"code","metadata":{"id":"ny8d3n9l1FTm","colab_type":"code","colab":{}},"source":["a_payload = []\n","b_payload = []\n","c_payload = []\n","d_payload = []\n","e_payload = []\n","\n","for i, subject in enumerate(records_df.index.get_level_values(0).unique()):\n","  base_data = {\"nick\": subject, \"decision\": 0, \"score\": 0}\n","  \n","  # A model\n","  base_data[\"score\"] = np.mean(records_df.loc[subject][\"AVALUE\"].values).item()\n","  base_data[\"decision\"] = 0 if base_data[\"score\"] < 0.4 else 1\n","  a_payload.append(base_data)\n","  \n","  # B model\n","  base_data[\"score\"] = b_predictions[i][0].item()\n","  base_data[\"decision\"] = 0 if base_data[\"score\"] < 0.4 else 1\n","  b_payload.append(base_data)\n","\n","  # C model\n","  base_data[\"score\"] = c_predictions[i][0].item()\n","  base_data[\"decision\"] = 0 if base_data[\"score\"] < 0.8 else 1\n","  c_payload.append(base_data)\n","  \n","  # D model\n","  base_data[\"score\"] = np.mean(records_df.loc[subject][\"DVALUE\"].values).item()\n","  base_data[\"decision\"] = 0 if base_data[\"score\"] < 0.3 else 1\n","  d_payload.append(base_data)\n","  \n","  # E model\n","  base_data[\"score\"] = e_predictions[i][0].item()\n","  base_data[\"decision\"] = 0 if base_data[\"score\"] < 0.6 else 1\n","  e_payload.append(base_data)\n","  \n","predictions_list = [a_payload, b_payload, c_payload, d_payload, e_payload]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uCovbL4eSf5O","colab_type":"text"},"source":["Post Results\n","==========="]},{"cell_type":"code","metadata":{"id":"CcJ8nPTQShu3","colab_type":"code","colab":{}},"source":["for i, predictions in enumerate(predictions_list):\n","  print(i)\n","  post_url_challenge1 = \"http://erisk.irlab.org/challenge-t1/submit/{}/{}\".format(TEAM_TOKEN, i)\n","  r = requests.post(post_url_challenge1, json=predictions)\n","  \n","  print(r.status_code)\n","  print(r.json())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mV4yKNw9vvRL","colab_type":"text"},"source":["ALL TOGETHER\n","============="]},{"cell_type":"code","metadata":{"id":"mz2hIAVFvui5","colab_type":"code","outputId":"a5f662f2-6e80-4480-8839-82db1037f07a","executionInfo":{"status":"error","timestamp":1560006446737,"user_tz":-120,"elapsed":794362,"user":{"displayName":"PABLO RAEZ GARCIA-RETAMERO","photoUrl":"","userId":"03506494368259470344"}},"colab":{"base_uri":"https://localhost:8080/","height":572}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"erisk_client.ipynb\n","\n","Automatically generated by Colaboratory.\n","\n","Original file is located at\n","    https://colab.research.google.com/drive/1lIuI4CqwgWBkbuhT-eMU6bACiLvU5vCE\n","\"\"\"\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","while True:\n","  import gc\n","\n","  import requests\n","  import pandas as pd\n","  import os\n","  import numpy as np\n","\n","  TEAM_TOKEN = '1f4c93c4e114d6e72706c31901101db7vv'\n","  SERVER_URL_TASK1 = 'http://erisk.irlab.org/challenge-t1e/getwritings/' + TEAM_TOKEN\n","  SERVER_URL_TASK2 = 'http://erisk.irlab.org/challenge-t2/getwritings/' + TEAM_TOKEN\n","\n","  SST_HOME='drive/My Drive/Colab Notebooks/Erisk2019/'\n","  PATH_SERVER_TASK1=SST_HOME+'datas/unoficcial server data/task1/test.csv'\n","  PATH_SERVER_TASK2=SST_HOME+'datas/unoficcial server data/task2/test.csv'\n","  single_num_prev = 0\n","\n","\n","  def ask_server_df(url):\n","    \"\"\"\n","    This function performs a get request to the servers and returns the data as a dataframe.\n","\n","    Returns None if any problem occurs.\n","    \"\"\"\n","    r = requests.get(url)\n","    if r.status_code == 200:\n","      df = pd.DataFrame(r.json())\n","      df = df.rename(index=str, columns={\"nick\": \"ID\", \"date\": \"DATE\", \"title\":\"TITLE\", \"redditor\":\"INFO\", \"content\":\"TEXT\"})\n","      df = df.set_index(['ID', 'DATE'])\n","      return df\n","    else:\n","      return None\n","\n","  df_t1 = ask_server_df(SERVER_URL_TASK1)\n","  df_t2 = ask_server_df(SERVER_URL_TASK2)\n","\n","  def append_to_df(path, df, copy=True):\n","    \"\"\"\n","    This function appends the new recorded data to an already existing csv. \n","\n","    If copy is set to True (default) it creates a copy of the csv files.\n","    \"\"\"\n","\n","    if os.path.exists(path):\n","      df2 = pd.read_csv(path, index_col=['ID', 'DATE'])\n","      if copy:\n","        df2.to_csv(path[:-4]+'_old'+path[-4:])\n","\n","      df = df.append(df2).drop_duplicates()\n","      df.to_csv(path)\n","\n","    else:\n","      df.to_csv(path)\n","\n","  if not df_t1 is None:\n","    append_to_df(PATH_SERVER_TASK1, df_t1, copy=False)\n","    \n","  if not df_t2 is None:\n","    append_to_df(PATH_SERVER_TASK2, df_t2, copy=False)\n","\n","  \"\"\"Load and Ready Data\n","  =================\n","  \"\"\"\n","\n","  records_df = pd.read_csv(PATH_SERVER_TASK1)\n","  records_df = records_df.set_index(['ID', 'DATE'])\n","  records_df[\"TITE\"] = records_df[\"TEXT\"] + records_df[\"TITLE\"]\n","\n","\n","  records_x_subject = []\n","  for subject in records_df.index.get_level_values(0).unique():\n","    records_x_subject.append(records_df.loc[subject][\"TITE\"].tolist())\n","\n","  records_x_simple = []\n","  for subject in records_x_subject:\n","    for writting in subject:\n","      records_x_simple.append(writting)\n","\n","  print(len(records_x_simple), len(records_x_subject))\n","  \n","  if len(records_x_simple) == single_num_prev:\n","    break\n","  \n","  single_num_prev = len(records_x_simple)\n","\n","  \"\"\"Tokenize\n","  ========\n","  \"\"\"\n","\n","  from keras.preprocessing.text import Tokenizer\n","  from keras.preprocessing.sequence import pad_sequences\n","  from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","  import nltk\n","  from nltk.corpus import stopwords\n","  nltk.download('stopwords')\n","\n","  stop_words=stopwords.words('english')\n","  # Define maximum vocabulary length\n","  MAX_WORDS = 5000\n","\n","  # We are using this function to clean the test set\n","  def tokenize_clean_text(text, tfidf=True, tokenizer=None, max_length=None, max_words=MAX_WORDS):\n","    \"\"\"\n","    This function is in charge of tokenizing the text it is given. It also cleans\n","    the text from stop-words, punctuation, and gives a special token to numbers.\n","\n","    :param text: The texts to tokenize in a bidimensional python array.\n","\n","    :returns: The tokenized and cleaned text in a bidimensional python array.\n","              The tokenizer used to preprocess the text.\n","              The maximum length used for padding.\n","    \"\"\"   \n","    # set [removed] as a special token\n","    text_removed = [t.replace(\"[removed]\", \"R3MOV3D\") for t in text]\n","\n","    # We remove the numbers\n","    cropped_numbers_text = [\" \".join([word if not word.isdigit() else \"\"\n","                                  for word in sentence.split()])\n","                                 for sentence in text_removed]\n","\n","    # Delete stopwords as well as every word less than 3 chars.\n","    cropped_numbers_stopw_text = [\" \".join([word if not (word in stop_words or len(word) <= 3) else \"\"\n","                                        for word in sentence.split()])\n","                                       for sentence in cropped_numbers_text]\n","\n","    if tfidf:\n","      vec = TfidfVectorizer(max_features=max_words)\n","      tfidf_mat = vec.fit_transform(cropped_numbers_stopw_text).toarray()\n","      tfid_words = vec.get_feature_names()\n","\n","      cropped_numbers_stopw_tfidf_text = [\" \".join([word if word in tfid_words else \"\"\n","                                              for word in sentence.split()])\n","                                              for sentence in cropped_numbers_stopw_text]\n","\n","    if tokenizer is None:\n","      tokenizer = Tokenizer(num_words=max_words) # They use 5k words too\n","      tokenizer.fit_on_texts(cropped_numbers_stopw_tfidf_text if tfidf else cropped_numbers_stopw_text)\n","    # We tokenize the sentences\n","    tokenized_text = tokenizer.texts_to_sequences(cropped_numbers_stopw_tfidf_text if tfidf else cropped_numbers_stopw_text)\n","\n","    if max_length == None:\n","      max_length = 0\n","      for sentence in tokenized_text:\n","        max_length = max_length if len(sentence) < max_length else len(sentence)\n","\n","    # Now we return the padded the sequences.\n","    return pad_sequences(tokenized_text, max_length), tokenizer, max_length, cropped_numbers_stopw_tfidf_text if tfidf else cropped_numbers_stopw_text\n","\n","  # load the tokenizer\n","  import joblib\n","  PATH_CHAR_TOKENIZER = SST_HOME + \"DL/char_tokenizer.pkl\"\n","  PATH_TOKENIZER = SST_HOME + \"DL/tokenizer.pkl\"\n","  tokenizer = joblib.load(PATH_TOKENIZER)\n","  char_tokenizer = joblib.load(PATH_CHAR_TOKENIZER)\n","\n","  records_x_token, _, _, results_x_clean = tokenize_clean_text(records_x_simple, tokenizer=tokenizer)\n","\n","  # the token length will be set to 50\n","  max_length = 50\n","  records_x_token_crop = records_x_token[:,-max_length:]\n","\n","  records_df[\"TOKENIZED\"] = records_x_token_crop.tolist()\n","\n","  results_x_char = [[c for c in instance] for instance in results_x_clean]\n","  # the char length will be set to 50\n","  max_char_length = 400\n","\n","  results_x_char_padded = [[\".\"]*(max_char_length - len(instance)) + instance if len(instance) < max_char_length else instance for instance in results_x_char]\n","  results_x_char_crop = [instance[-max_char_length:] for instance in results_x_char_padded]\n","\n","  def tokenize_chars(text, tokenizer=None, max_length=max_char_length):\n","    if tokenizer is None:\n","      tokenizer = Tokenizer() # They use 5k words too\n","      tokenizer.fit_on_texts(text)\n","    # We tokenize the sentences\n","    tokenized_text = tokenizer.texts_to_sequences(text)\n","\n","    for i, t in enumerate(tokenized_text):\n","      if len(t) < max_length:\n","        tokenized_text[i] = [1] * (max_length - len(t)) + tokenized_text[i]\n","\n","    return tokenized_text, tokenizer\n","\n","  results_x_char_crop_token, _ = tokenize_chars(results_x_char_crop, char_tokenizer)\n","\n","  records_df[\"TOK_CHAR\"] = results_x_char_crop_token\n","\n","  \"\"\"Preprocess with A and D models\n","  ==========================\n","  \"\"\"\n","\n","  A_MODELS_PATH = SST_HOME + \"DL/models/A/best_models/\"\n","  B_MODELS_PATH = SST_HOME + \"DL/models/B/best_models/\"\n","  C_MODELS_PATH = SST_HOME + \"DL/models/C/best_models/\"\n","  D_MODELS_PATH = SST_HOME + \"DL/models/D/best_models/\"\n","  E_MODELS_PATH = SST_HOME + \"DL/models/E/best_models/\"\n","\n","\n","  A_MODEL_PATH = A_MODELS_PATH + \"Copia de load_emb_False_num_classes_1_emb_size_300_trainable_emb_True_cnn_size_128_cnn_filter_3_rnn_size_None_cell_type_LSTM_bidirectional_False_attention_False_dropout_0.5_dnn_size_32_batch_size_1024\"\n","  B_MODEL_PATH = B_MODELS_PATH + \"Copia de load_emb_False_num_classes_1_emb_size_300_trainable_emb_True_cnn_size_None_cnn_filter_3_rnn_size_64_cell_type_GRU_bidirectional_True_attention_False_dropout_0.5_dnn_size_32_batch_size_None\"\n","  C_MODEL_PATH = C_MODELS_PATH + \"Copia de load_emb_False_num_classes_1_emb_size_None_trainable_emb_False_cnn_size_None_cnn_filter_3_rnn_size_64_cell_type_LSTM_bidirectional_True_attention_False_dropout_0.5_dnn_size_32_batch_size_1\"\n","  D_MODEL_PATH = D_MODELS_PATH + \"Copia de load_emb_False_num_classes_1_emb_size_50_trainable_emb_True_cnn_size_128_cnn_filter_10_rnn_size_None_cell_type_LSTM_bidirectional_False_attention_False_dropout_0.5_dnn_size_64_batch_size_1024\"\n","  E_MODEL_PATH = E_MODELS_PATH + \"Copia de load_emb_False_num_classes_1_emb_size_None_trainable_emb_False_cnn_size_None_cnn_filter_3_rnn_size_64_cell_type_GRU_bidirectional_False_attention_False_dropout_0.5_dnn_size_32_batch_size_1\"\n","\n","  from keras.models import load_model\n","  from keras.models import Sequential\n","\n","  a_model = load_model(A_MODEL_PATH)\n","  a_model.layers.pop()\n","  a_model.layers.pop()\n","  # We pop two layers, being them the output and the last Dropout layers\n","  c_pre_model = Sequential()\n","\n","  for layer in a_model.layers:\n","    c_pre_model.add(layer)\n","\n","  d_model = load_model(D_MODEL_PATH)\n","  d_model.layers.pop()\n","  d_model.layers.pop()\n","  # We pop two layers, being them the output and the last dropout ones\n","  e_pre_model = Sequential()\n","\n","  for layer in d_model.layers:\n","    e_pre_model.add(layer)\n","\n","  instances = np.array(records_df[\"TOKENIZED\"].tolist())\n","  char_instances = np.array(records_df[\"TOK_CHAR\"].tolist())\n","\n","  embedded_text = c_pre_model.predict(instances)\n","  embedded_chars = e_pre_model.predict(char_instances)\n","\n","  records_df[\"EMBEDDINGS\"] = embedded_text.tolist()\n","  records_df[\"CHAR_EMBED\"] = embedded_chars.tolist()\n","\n","  records_subjects_stacked = []\n","  records_subjects_embeddings = []\n","  records_subjects_char_embeddings = []\n","\n","  for subject in records_df.index.get_level_values(0).unique():\n","    # B model\n","    for writting in records_df.loc[subject][\"TOKENIZED\"].tolist():\n","      while 0 in writting:\n","        writting.remove(0)\n","      writting = writting[-350:]\n","      if len(writting) < 350:\n","        writting = [0]*(350 - len(writting)) + writting\n","    records_subjects_stacked.append(writting)\n","    records_subjects_embeddings.append(records_df.loc[subject][\"EMBEDDINGS\"].tolist())\n","    records_subjects_char_embeddings.append(records_df.loc[subject][\"CHAR_EMBED\"].tolist())\n","\n","  records_df[\"TOKENIZED\"] = records_x_token_crop.tolist()\n","\n","  \"\"\"Load Models\n","  ===========\n","  \"\"\"\n","\n","  # load_models\n","  a_model = load_model(A_MODEL_PATH)\n","  b_model = load_model(B_MODEL_PATH)\n","  c_model = load_model(C_MODEL_PATH)\n","  d_model = load_model(D_MODEL_PATH)\n","  e_model = load_model(E_MODEL_PATH)\n","\n","  records_df[\"AVALUE\"] = np.nan\n","  records_df[\"BVALUE\"] = np.nan\n","  records_df[\"CVALUE\"] = np.nan\n","  records_df[\"DVALUE\"] = np.nan\n","  records_df[\"EVALUE\"] = np.nan\n","\n","  # Predictions\n","\n","  # A model\n","  records_df[\"AVALUE\"] = a_model.predict(np.array(records_df[\"TOKENIZED\"].values.tolist()))\n","  print(\"A done\")\n","\n","  # B model\n","  b_predictions = b_model.predict(np.array(records_subjects_stacked))\n","  print(\"B done\")\n","\n","  # C model\n","  c_predictions = [c_model.predict(np.array([s_c])) for s_c in records_subjects_embeddings]\n","  print(\"C done\")\n","\n","  # D model\n","  records_df[\"DVALUE\"] = d_model.predict(np.array(records_df[\"TOK_CHAR\"].values.tolist()))\n","  print(\"D done\")\n","\n","  # E model\n","  e_predictions = [e_model.predict(np.array([s_e])) for s_e in records_subjects_char_embeddings]\n","  print(\"E done\")\n","\n","  \"\"\"Subjects results\n","  =============\n","  \"\"\"\n","\n","  a_payload = []\n","  b_payload = []\n","  c_payload = []\n","  d_payload = []\n","  e_payload = []\n","\n","  for i, subject in enumerate(records_df.index.get_level_values(0).unique()):\n","    base_data = {\"nick\": subject, \"decision\": 0, \"score\": 0}\n","\n","    # A model\n","    base_data[\"score\"] = np.mean(records_df.loc[subject][\"AVALUE\"].values).item()\n","    base_data[\"decision\"] = 0 if base_data[\"score\"] < 0.4 else 1\n","    a_payload.append(base_data.copy())\n","\n","    # B model\n","    base_data[\"score\"] = b_predictions[i][0].item()\n","    base_data[\"decision\"] = 0 if base_data[\"score\"] < 0.1 else 1\n","    b_payload.append(base_data.copy())\n","\n","    # C model\n","    base_data[\"score\"] = c_predictions[i][0].item()\n","    base_data[\"decision\"] = 0 if base_data[\"score\"] < 0.9 else 1\n","    c_payload.append(base_data.copy())\n","\n","    # D model\n","    base_data[\"score\"] = np.mean(records_df.loc[subject][\"DVALUE\"].values).item()\n","    base_data[\"decision\"] = 0 if base_data[\"score\"] < 0.3 else 1\n","    d_payload.append(base_data.copy())\n","\n","    # E model\n","    base_data[\"score\"] = e_predictions[i][0].item()\n","    base_data[\"decision\"] = 0 if base_data[\"score\"] < 0.6 else 1\n","    e_payload.append(base_data.copy())\n","\n","  predictions_list = [a_payload, b_payload, c_payload, d_payload, e_payload]\n","\n","  \"\"\"Post Results\n","  ===========\n","  \"\"\"\n","  \n","  with open(SST_HOME+\"kiwi.txt\", \"w\") as results_file:\n","    for i, predictions in enumerate(predictions_list):\n","      print(i)\n","      post_url_challenge1 = \"http://erisk.irlab.org/challenge-t1e/submit/{}/{}\".format(TEAM_TOKEN, i)\n","      print(post_url_challenge1)\n","      results_file.write(str(predictions))\n","      \n","      r = requests.post(post_url_challenge1, json=predictions)\n","\n","      print(r.status_code)\n","      print(r.json())\n","          \n","  #%reset -f"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","149092 628\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","A done\n","B done\n","C done\n","D done\n","E done\n","0\n","http://erisk.irlab.org/challenge-t1e/submit/1f4c93c4e114d6e72706c31901101db7vv/0\n","200\n","[]\n","1\n","http://erisk.irlab.org/challenge-t1e/submit/1f4c93c4e114d6e72706c31901101db7vv/1\n","200\n","[]\n","2\n","http://erisk.irlab.org/challenge-t1e/submit/1f4c93c4e114d6e72706c31901101db7vv/2\n","200\n","[]\n","3\n","http://erisk.irlab.org/challenge-t1e/submit/1f4c93c4e114d6e72706c31901101db7vv/3\n","200\n","[]\n","4\n","http://erisk.irlab.org/challenge-t1e/submit/1f4c93c4e114d6e72706c31901101db7vv/4\n","200\n","[]\n","149092 628\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B__VjGoaJnlL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}